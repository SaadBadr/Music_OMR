{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier  # MLP is an NN\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils  # If you are unable to install this library, ask the TA; we only need this in extract_hsv_histogram.\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "# Depending on library versions on your system, one of the following imports \n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_datasets = r'dataset/'\n",
    "target_img_size = (64, 64) # fix image size because classification algorithms THAT WE WILL USE HERE expect that\n",
    "\n",
    "# We are going to fix the random seed to make our experiments reproducible \n",
    "# since some algorithms use pseudorandom generators\n",
    "random_seed = 42  \n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I - Feature Extraction\n",
    "\n",
    "In this part, we are going to implement two functions. Each one will extract a different set of features from the image. The two sets are:\n",
    "\n",
    "1. Histogram of Gradients (HoG) features\n",
    "2. Raw pixels (basically, not doing any feature extraction and just supplying the input image to the classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hog_features(img):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    You won't implement anything in this function. You just need to understand it \n",
    "    and understand its parameters (i.e win_size, cell_size, ... etc)\n",
    "    \"\"\"\n",
    "    img = cv2.resize(img, target_img_size)\n",
    "    win_size = (32, 32)\n",
    "    cell_size = (4, 4)\n",
    "    block_size_in_cells = (2, 2)\n",
    "    \n",
    "    block_size = (block_size_in_cells[1] * cell_size[1], block_size_in_cells[0] * cell_size[0])\n",
    "    block_stride = (cell_size[1], cell_size[0])\n",
    "    nbins = 9  # Number of orientation bins\n",
    "    hog = cv2.HOGDescriptor(win_size, block_size, block_stride, cell_size, nbins)\n",
    "    h = hog.compute(img)\n",
    "    h = h.flatten()\n",
    "    return h.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_raw_pixels(img):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    The classification algorithms we are going to use expect the input to be a vector not a matrix. \n",
    "    This is because they are general purpose and don't work only on images.\n",
    "    CNNs, on the other hand, expect matrices since they operate on images and exploit the \n",
    "    arrangement of pixels in the 2-D space.\n",
    "    \n",
    "    So, what we only need to do in this function is to resize and flatten the image.\n",
    "    \"\"\"\n",
    "    img = cv2.resize(img, target_img_size)\n",
    "\n",
    "    img = img.flatten()\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(img, feature_set='hog'):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    Given either 'hsv_hist', 'hog', 'raw', call the respective function and return its output\n",
    "    \"\"\"\n",
    "    if feature_set == 'hog':\n",
    "        return extract_hog_features(img)\n",
    "\n",
    "    if feature_set == 'raw':\n",
    "        return extract_raw_pixels(img)\n",
    "\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will extract the features and the label of each image in our dataset and save it in RAM. We normally don't save datasets in RAM, but this dataset is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(feature_set='hog'):\n",
    "    features = []\n",
    "    labels = []\n",
    "    types = os.listdir(path_to_datasets)\n",
    "    for t in types:\n",
    "        path_to_dataset = f'{path_to_datasets}/{t}'\n",
    "        img_filenames = os.listdir(path_to_dataset)\n",
    "        for i, fn in enumerate(img_filenames):\n",
    "            if fn.split('.')[-1] != 'jpg' and  fn.split('.')[-1] != 'png':\n",
    "                continue\n",
    "\n",
    "            labels.append(t)\n",
    "\n",
    "            path = os.path.join(path_to_dataset, fn)\n",
    "            img = cv2.imread(path)\n",
    "            features.append(extract_features(img, feature_set))\n",
    "            # if(i == 100): \n",
    "            #     break\n",
    "            # show an update every 1,000 images\n",
    "            if i > 0 and i % 1000 == 0:\n",
    "                print(\"[INFO] processed {}/{}\".format(i, len(img_filenames)))\n",
    "        \n",
    "    return features, labels        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II - Classification\n",
    "\n",
    "In this part, we will test the classification performance of SVM, KNN, & NNs given our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO understand the hyperparameters of each classifier\n",
    "classifiers = {\n",
    "    'SVM': svm.LinearSVC(random_state=random_seed),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=7),\n",
    "    'NN': MLPClassifier(solver='sgd', random_state=random_seed, hidden_layer_sizes=(500,), max_iter=20, verbose=1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will test all our classifiers on a specific feature set\n",
    "def run_experiment(feature_set):\n",
    "    \n",
    "    # Load dataset with extracted features\n",
    "    print('Loading dataset. This will take time ...')\n",
    "    features, labels = load_dataset(feature_set)\n",
    "    print(len(labels))\n",
    "    print('Finished loading dataset.')\n",
    "    # Since we don't want to know the performance of our classifier on images it has seen before\n",
    "    # we are going to withhold some images that we will test the classifier on after training \n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "        features, labels, test_size=0.2, random_state=random_seed)\n",
    "    \n",
    "    for model_name, model in classifiers.items():\n",
    "        print('############## Training', model_name, \"##############\")\n",
    "        # Train the model only on the training features\n",
    "        model.fit(train_features, train_labels)\n",
    "        \n",
    "        # Test the model on images it hasn't seen before\n",
    "        accuracy = model.score(test_features, test_labels)\n",
    "        \n",
    "        print(model_name, 'accuracy:', accuracy*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we see how each classifier and each feature set performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading dataset. This will take time ...\n",
      "642\n",
      "Finished loading dataset.\n",
      "############## Training SVM ##############\n",
      "SVM accuracy: 100.0 %\n",
      "############## Training KNN ##############\n",
      "KNN accuracy: 94.57364341085271 %\n",
      "############## Training NN ##############\n",
      "Iteration 1, loss = 2.46147119\n",
      "Iteration 2, loss = 1.92034379\n",
      "Iteration 3, loss = 1.44936613\n",
      "Iteration 4, loss = 1.03428730\n",
      "Iteration 5, loss = 0.72181444\n",
      "Iteration 6, loss = 0.50891431\n",
      "Iteration 7, loss = 0.37008317\n",
      "Iteration 8, loss = 0.28187686\n",
      "Iteration 9, loss = 0.21837134\n",
      "Iteration 10, loss = 0.17567735\n",
      "Iteration 11, loss = 0.14540049\n",
      "Iteration 12, loss = 0.12313771\n",
      "Iteration 13, loss = 0.10774574\n",
      "Iteration 14, loss = 0.09391268\n",
      "Iteration 15, loss = 0.08399710\n",
      "Iteration 16, loss = 0.07482252\n",
      "Iteration 17, loss = 0.06788022\n",
      "Iteration 18, loss = 0.06205430\n",
      "Iteration 19, loss = 0.05725713\n",
      "Iteration 20, loss = 0.05311725\n",
      "NN accuracy: 96.89922480620154 %\n"
     ]
    }
   ],
   "source": [
    "run_experiment('hog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading dataset. This will take time ...\n",
      "642\n",
      "Finished loading dataset.\n",
      "############## Training SVM ##############\n",
      "SVM accuracy: 100.0 %\n",
      "############## Training KNN ##############\n",
      "KNN accuracy: 96.89922480620154 %\n",
      "############## Training NN ##############\n",
      "Iteration 1, loss = 31.91787267\n",
      "Iteration 2, loss = 30.04082853\n",
      "Iteration 3, loss = 35.55087287\n",
      "Iteration 4, loss = 37.45742229\n",
      "Iteration 5, loss = 21.66684662\n",
      "Iteration 6, loss = 31.78272027\n",
      "Iteration 7, loss = 41.22507397\n",
      "Iteration 8, loss = 49.32062551\n",
      "Iteration 9, loss = 56.24122713\n",
      "Iteration 10, loss = 61.16584654\n",
      "Iteration 11, loss = 65.05090292\n",
      "Iteration 12, loss = 67.95016587\n",
      "Iteration 13, loss = 70.10960998\n",
      "Iteration 14, loss = 71.70790389\n",
      "Iteration 15, loss = 72.84293295\n",
      "Iteration 16, loss = 73.69923057\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "NN accuracy: 20.930232558139537 %\n"
     ]
    }
   ],
   "source": [
    "run_experiment('raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifiers list now has models trained on the last feature set you ran an experiment on. You can play around with it checking the probability it gives to each label, given an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "test_img_path = r'test.png'\n",
    "img = cv2.imread(test_img_path)\n",
    "features = extract_features(img, 'raw')  # be careful of the choice of feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NN predicition: ['natural']\nNN probabilities:  [[0.08173966 0.07107601 0.07555108 0.07885446 0.08382439 0.07333052\n  0.08306647 0.07768309 0.0700194  0.08118711 0.07988207 0.0675837\n  0.07620204]]\n"
     ]
    }
   ],
   "source": [
    "nn = classifiers['NN']\n",
    "nn_probabilities =  nn.predict_proba([features])\n",
    "nn_prediction =  nn.predict([features])\n",
    "print('NN predicition:', nn_prediction)\n",
    "print('NN probabilities: ', nn_probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "KNN predicition: ['sharp']\nKNN probabilities:  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "knn = classifiers['KNN']\n",
    "knn_probabilities =  knn.predict_proba([features])\n",
    "knn_prediction =  knn.predict([features])\n",
    "print('KNN predicition:', knn_prediction)\n",
    "print('KNN probabilities: ', knn_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SVM predicition: ['sharp']\n"
     ]
    }
   ],
   "source": [
    "svm = classifiers['SVM']\n",
    "svm_prediction =  knn.predict([features])\n",
    "print('SVM predicition:', svm_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}